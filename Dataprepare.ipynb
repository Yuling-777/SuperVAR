{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22ec17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"daehoyang/flickr2k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4de936",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "if os.path.exists('/content/VAR'): os.chdir('/content/VAR')\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "# we recommend using imagenet-512-d36 model to do the in-painting & out-painting & class-condition editing task\n",
    "MODEL_DEPTH = 36    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30, 36}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = '/model_path/var/vae_ch160v4096z32.pth', f'/model_path/var/var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
